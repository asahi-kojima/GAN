{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167cee4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1b46400ba425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models import vgg19\n",
    "import torchvision.transforms as transforms\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ade54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ディレクトリ 構成\n",
    "root\n",
    "    | - input\n",
    "    |        | - annotation\n",
    "    |        | - cat_face\n",
    "    |                   | - demo\n",
    "    |                   | - test\n",
    "    |                   | - train\n",
    "    |        | - images\n",
    "    |                   | - cat\n",
    "    |\n",
    "    | - outputs\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b38492",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = './'\n",
    "input_dir = osp.join(ROOT, 'input')\n",
    "output_dir = osp.join(ROOT, 'output')\n",
    "os.makedirs(input_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064fa6c",
   "metadata": {},
   "source": [
    "# データのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz -P ./input/\n",
    "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz -P ./input/\n",
    "\n",
    "!tar -zxvf ./input/images.tar.gz -C ./input\n",
    "!tar -zxvf ./input/annotations.tar.gz -C ./input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./input/images/cat\n",
    "!mv ./input/images/*.jpg ./input/images/cat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像を保存するディレクトリ\n",
    "image_dir = osp.join(input_dir, 'images/cat')\n",
    "# ラベルデータを保存するディレクトリ\n",
    "annotations_dir = osp.join(input_dir, 'annotations')\n",
    "# ラベルのリストファイルのパス\n",
    "list_path = osp.join(annotations_dir, 'list.txt')\n",
    "\n",
    "# データセットのラベル名\n",
    "cols = ['file_name', 'class_id', 'species', 'breed_id']\n",
    "\n",
    "#テキストファイルからデータの基礎情報の読み込み\n",
    "labels = []\n",
    "with open(list_path, 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        labels.append(line.split(' '))\n",
    "        \n",
    "labels_df = pd.DataFrame(labels, columns=cols)\n",
    "labels_df = labels_df[['file_name', 'species']]\n",
    "\n",
    "cat_label_df = labels_df[labels_df.species=='1']\n",
    "cat_label_df = cat_label_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ffe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットのディレクトリ\n",
    "dataset_dir = osp.join(input_dir, 'cat_face')\n",
    "# データセットの学習データを保存するディレクトリ\n",
    "train_dir = osp.join(dataset_dir, 'train')\n",
    "# データセットのテストデータを保存するディレクトリ\n",
    "test_dir = osp.join(dataset_dir, 'test')\n",
    "# デモ用のデータを保存するディレクトリ\n",
    "demo_dir = osp.join(dataset_dir, 'demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 19930124\n",
    "random_crop_times = 4\n",
    "# クロップする画像のサイズ\n",
    "crop_size = (128, 128)\n",
    "dataset_name = 'cat_face'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データクロップ用の関数\n",
    "def random_crop(image, crop_size):\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    top = np.random.randint(0, h - crop_size[0])\n",
    "    left = np.random.randint(0, w - crop_size[1])\n",
    "\n",
    "    bottom = top + crop_size[0]\n",
    "    right = left + crop_size[1]\n",
    "\n",
    "    image = image[top:bottom, left:right, :]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c7084",
   "metadata": {},
   "source": [
    "## 猫の名前からテスト用とデモ用を取り出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(cat_label_df, test_size=5, random_state=seed)\n",
    "train_df, demo_df = train_test_split(train_df, test_size=1, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c6560",
   "metadata": {},
   "source": [
    "## 上で分離した訓練用のデータに対し、クロップを適用してデータを加工・増量する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習に用いる画像\n",
    "for item in tqdm(train_df.file_name, total=len(train_df)):\n",
    "    image_name = '{}.jpg'.format(item)\n",
    "    image_path = osp.join(image_dir, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, _ = image.shape\n",
    "    # 画像のサイズが小さい時は対象から除外する。\n",
    "    if (h < crop_size[0]) | (w < crop_size[1]):\n",
    "        print('{} size is invalid. h: {},  w: {}'.format(image_name, h, w))\n",
    "        continue\n",
    "    #ランダムクロップ分だけ作る\n",
    "    for num in range(random_crop_times):\n",
    "        cropped_image = random_crop(image, crop_size=crop_size)\n",
    "        image_save_name = '{}_{:03}.jpg'.format(item, num)\n",
    "        cropped_image_save_path = osp.join(train_dir, image_save_name)\n",
    "        os.makedirs(osp.dirname(cropped_image_save_path), exist_ok=True)\n",
    "        cv2.imwrite(cropped_image_save_path, cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93500b",
   "metadata": {},
   "source": [
    "## テスト用とデモ用の画像を専用のディレクトリに移しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639cd76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習の進度確認の画像\n",
    "for item in test_df.file_name:\n",
    "    image_name = '{}.jpg'.format(item)\n",
    "    image_path = osp.join(image_dir, image_name)\n",
    "    \n",
    "    image_save_path = osp.join(test_dir, image_name)\n",
    "    os.makedirs(osp.dirname(image_save_path), exist_ok=True)\n",
    "    shutil.copy(image_path, image_save_path)\n",
    "    \n",
    "# 学習後に超解像を試す画像\n",
    "for item in demo_df.file_name:\n",
    "    image_name = '{}.jpg'.format(item)\n",
    "    image_path = osp.join(image_dir, image_name)\n",
    "    \n",
    "    image_save_path = osp.join(demo_dir, image_name)\n",
    "    os.makedirs(osp.dirname(image_save_path), exist_ok=True)\n",
    "    shutil.copy(image_path, image_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8108719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('学習に用いる画像（最初の数枚）')\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_paths = glob(osp.join(train_dir, '*'))\n",
    "for num, path in enumerate(train_paths[0:5], 1):\n",
    "    image = cv2.imread(path)\n",
    "    plt.subplot(1, 5, num)\n",
    "    plt.imshow(image[:,:,::-1])\n",
    "plt.show()\n",
    "\n",
    "print('学習の進度確認の画像')\n",
    "plt.figure(figsize=(12, 4))\n",
    "test_paths = glob(osp.join(test_dir, '*'))\n",
    "for num, path in enumerate(test_paths, 1):\n",
    "    image = cv2.imread(path)\n",
    "    plt.subplot(1, 5, num)\n",
    "    plt.imshow(image[:,:,::-1])\n",
    "plt.show()\n",
    "\n",
    "print('学習後に超解像を試す画像')\n",
    "plt.figure(figsize=(12, 4))\n",
    "demo_paths = glob(osp.join(demo_dir, '*'))\n",
    "for num, path in enumerate(demo_paths, 1):\n",
    "    image = cv2.imread(path)\n",
    "    plt.subplot(1, 5, num)\n",
    "    plt.imshow(image[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed07241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    学習のためのDatasetクラス\n",
    "    32×32の低解像度の本物画像と、128×128の本物画像を出力する\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_dir, hr_shape):\n",
    "        hr_height, hr_width = hr_shape\n",
    "        \n",
    "        # 低解像度の画像を取得するための処理\n",
    "        self.lr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)])\n",
    "\n",
    "        # 高像度の画像を取得するための処理\n",
    "        self.hr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)])\n",
    "        \n",
    "        self.files = sorted(glob(osp.join(dataset_dir, '*')))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img_lr = self.lr_transform(img)\n",
    "        img_hr = self.hr_transform(img)\n",
    "        \n",
    "        return {'lr': img_lr, 'hr': img_hr}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed52264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, dataset_dir):\n",
    "        self.hr_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean, std)])\n",
    "        self.files = sorted(glob(osp.join(dataset_dir, '*')))\n",
    "    \n",
    "    def lr_transform(self, img, img_size):\n",
    "        img_width, img_height = img_size\n",
    "        self.__lr_transform = transforms.Compose([\n",
    "            transforms.Resize((img_height // 4, \n",
    "                               img_width // 4), \n",
    "                               Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)])\n",
    "        img = self.__lr_transform(img)\n",
    "        return img\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img_size = img.size\n",
    "        img_lr = self.lr_transform(img, img_size)\n",
    "        img_hr = self.hr_transform(img)        \n",
    "        return {'lr': img_lr, 'hr': img_hr}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df15a5",
   "metadata": {},
   "source": [
    "# parts of ESRGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e74591",
   "metadata": {},
   "source": [
    "## 生成器の準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93ab35",
   "metadata": {},
   "source": [
    "## Dense Residual Blockの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseResidualBlock(nn.Module):\n",
    "    def __init__(self, filters, res_scale = 0.2):\n",
    "        super().__init__()\n",
    "        self.res_scale = res_scale\n",
    "        \n",
    "        def block(in_features, non_linearity = True):\n",
    "            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias = True)]\n",
    "            if non_linearity:\n",
    "                layers += [nn.LeakyReLU()]\n",
    "            return nn.Sequential(*layers)\n",
    "    \n",
    "        self.b1 = block(in_features = 1 * filters)\n",
    "        self.b2 = block(in_features = 2 * filters)\n",
    "        self.b3 = block(in_features = 3 * filters)\n",
    "        self.b4 = block(in_features = 4 * filters)\n",
    "        self.b5 = block(in_features = 5 * filters, non_linearity = False)\n",
    "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        for block in self.blocks:\n",
    "            out = block(inputs)\n",
    "            inputs = torch.cat([inputs, out], dim = 1)\n",
    "        return out.mul(self.res_scale) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603511e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サイズの確認\n",
    "x = torch.randn(10, 64, 5, 5)\n",
    "D = DenseResidualBlock(filters = 64)\n",
    "D(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72574ecd",
   "metadata": {},
   "source": [
    "## Residual in Residual Blockの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72871a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual-blockの再帰\n",
    "class ResidualInResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, filters, res_scale=0.2):\n",
    "        super().__init__()\n",
    "        self.res_scale = res_scale\n",
    "        self.dense_blocks = nn.Sequential(\n",
    "            DenseResidualBlock(filters), \n",
    "            DenseResidualBlock(filters), \n",
    "            DenseResidualBlock(filters)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dense_blocks(x).mul(self.res_scale) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サイズの確認\n",
    "DD = ResidualInResidualDenseBlock(filters = 64)\n",
    "DD(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30403f9a",
   "metadata": {},
   "source": [
    "## 構築してきたパーツを組み合わせて生成器を構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Conv2D\n",
    "RinRD-Block\n",
    "     .\n",
    "     .\n",
    "RinRD-Block\n",
    "Conv2D\n",
    "|---------------|\n",
    "|Conve2D     |\n",
    "|LeaklyReLU |\n",
    "|Pixelshuffle |\n",
    "|---------------|\n",
    "     .\n",
    "     .\n",
    "|---------------|\n",
    "|Conve2D     |\n",
    "|LeaklyReLU |\n",
    "|Pixelshuffle |\n",
    "|---------------|\n",
    "Conv2D\n",
    "LeaklyReLU\n",
    "Conv2D\n",
    "\"\"\"\n",
    "\n",
    "class GeneratorRRDB(nn.Module):\n",
    "    def __init__(self, channels, filters = 64, num_res_blocks = 16, num_upsample = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channels, filters, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters)  for _ in range(num_res_blocks)])\n",
    "        self.conv2 = nn.Conv2d(filters, filters, kernel_size = 3, stride = 1, padding = 1)\n",
    "        \n",
    "        upsample_layers = []\n",
    "        \n",
    "        for _ in range(num_upsample):\n",
    "            upsample_layers += [\n",
    "                nn.Conv2d(filters, filters * 4, kernel_size = 3, stride = 1, padding = 1),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.PixelShuffle(upscale_factor = 2),\n",
    "            ]\n",
    "        self.upsampling = nn.Sequential(*upsample_layers)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(filters, filters, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(filters, channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        #print(1,out1.shape)\n",
    "        out = self.res_blocks(out1)\n",
    "        #print(2,out.shape)\n",
    "        out2 = self.conv2(out)\n",
    "        #print(3,out2.shape)\n",
    "        out = torch.add(out1, out2)\n",
    "        #print(4,out.shape)\n",
    "        out = self.upsampling(out)\n",
    "        #print(5,out.shape)\n",
    "        out = self.conv3(out)\n",
    "        #print(6,out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8996458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サイズの確認\n",
    "# ex : (10,3,32,32) --> (10, 3, 128, 128)\n",
    "y = torch.randn(10,3,32,32)\n",
    "DDD = GeneratorRRDB(3, filters = 64, num_res_blocks = 23)\n",
    "DDD(y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ac851",
   "metadata": {},
   "source": [
    "## 特徴量抽出層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c84f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    vgg19を応用した特徴量抽出器\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg19_model = vgg19(pretrained = True)\n",
    "        self.vgg19_54 = nn.Sequential(*list(vgg19_model.features.children())[:35])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg19_54(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex : (10, 3, 128, 128) --> (10, 512, 8,8)\n",
    "y = torch.randn(10,3,32,32)\n",
    "FE = FeatureExtractor()\n",
    "FE(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e697c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(10,3,32,32)\n",
    "(y-y.mean(0, keepdim=True)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e0a40",
   "metadata": {},
   "source": [
    "## 識別器の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "    \n",
    "        def discriminator_block(in_filters, out_filters, first_block = False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_filters, \n",
    "                                    out_filters, \n",
    "                                    kernel_size = 3, \n",
    "                                    stride = 1, \n",
    "                                    padding = 1))\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace = True))\n",
    "            layers.append(nn.Conv2d(out_filters, \n",
    "                                    out_filters, \n",
    "                                    kernel_size = 3, \n",
    "                                    stride = 2, \n",
    "                                    padding = 1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace = True))\n",
    "            return layers\n",
    "        \n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            #print(discriminator_block(in_filters, out_filters,  first_block=(i == 0)))\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "        \n",
    "        layers.append(nn.Conv2d(out_filters,  1, kernel_size = 3, stride = 1,  padding = 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8248246",
   "metadata": {},
   "source": [
    "## 構築してきたパーツを組み合わせてESRGANを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e354b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESRGAN():\n",
    "    def __init__(self, opt):\n",
    "        # 生成器・識別器の設定\n",
    "        self.generator = GeneratorRRDB(opt.channels, filters = 64, num_res_blocks = opt.residual_blocks).to(opt.device)\n",
    "        self.discriminator = Discriminator(input_shape = (opt.channels, *hr_shape)).to(opt.device)\n",
    "\n",
    "        # 特徴量抽出器の設定\n",
    "        self.feature_extractor = FeatureExtractor().to(opt.device)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "        # 損失関数の設定\n",
    "        self.criterion_GAN = nn.BCEWithLogitsLoss().to(opt.device)\n",
    "        self.criterion_content = nn.L1Loss().to(opt.device)\n",
    "        self.criterion_pixel = nn.L1Loss().to(opt.device)\n",
    "\n",
    "        # オプティマイザーの設定\n",
    "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr = opt.lr, betas = (opt.b1, opt.b2))\n",
    "        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr = opt.lr, betas = (opt.b1, opt.b2))\n",
    "\n",
    "        # デバイスとテンソル型の定義\n",
    "        self.Tensor = torch.Tensor\n",
    "        self.dev = opt.device\n",
    "        \n",
    "    #==================================================================================\n",
    "    \n",
    "    def pre_train(self, imgs, batch_num):\n",
    "        imgs_lr = imgs['lr'].type(torch.Tensor).to(self.dev)\n",
    "        imgs_hr = imgs['hr'].type(torch.Tensor).to(self.dev)\n",
    "\n",
    "        valid = torch.tensor(np.ones((imgs_lr.size(0), *self.discriminator.output_shape)), requires_grad=False).to(self.dev)\n",
    "        fake = torch.tensor(np.zeros((imgs_lr.size(0), *self.discriminator.output_shape)), requires_grad=False).to(self.dev)\n",
    "\n",
    "        # 勾配初期化\n",
    "        self.optimizer_G.zero_grad()\n",
    "\n",
    "        # 低解像度 --> 高解像度を実行し、ピクセル単位の損失計算\n",
    "        gen_hr = self.generator(imgs_lr)\n",
    "        loss_pixel = self.criterion_pixel(gen_hr, imgs_hr)\n",
    "\n",
    "        # 画素単位の損失であるloss_pixelで事前学習を行う\n",
    "        loss_pixel.backward()\n",
    "        self.optimizer_G.step()\n",
    "        train_info = {'epoch': epoch, 'batch_num': batch_num, 'loss_pixel': loss_pixel.item()}\n",
    "        \"\"\"\n",
    "        if batch_num == 1:\n",
    "            sys.stdout.write('\\n{}'.format(train_info))\n",
    "        else:\n",
    "            sys.stdout.write('\\r{}'.format('\\t'*20))\n",
    "            sys.stdout.write('\\r{}'.format(train_info))\n",
    "        \"\"\"\n",
    "        \n",
    "        sys.stdout.write('\\r{}'.format('\\t'*20))\n",
    "        sys.stdout.write('\\r{}'.format(train_info))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    #============================================================================\n",
    "    \n",
    "    def train(self, imgs, batch_num):\n",
    "        imgs_lr = imgs['lr'].type(self.Tensor).to(self.dev)\n",
    "        imgs_hr = imgs['hr'].type(self.Tensor).to(self.dev)\n",
    "\n",
    "        ################### \n",
    "        #####生成器の損失#####\n",
    "        ###################\n",
    "        # 正解ラベル\n",
    "        valid = torch.tensor(np.ones((imgs_lr.size(0), *self.discriminator.output_shape)), requires_grad = False).to(self.dev)\n",
    "        fake = torch.tensor(np.zeros((imgs_lr.size(0), *self.discriminator.output_shape)),requires_grad = False).to(self.dev)\n",
    "\n",
    "        # 低解像度 --> 高解像度\n",
    "        self.optimizer_G.zero_grad()\n",
    "        gen_hr = self.generator(imgs_lr)\n",
    "\n",
    "        # (1)ピクセル単位の損失計算\n",
    "        loss_pixel = self.criterion_pixel(gen_hr, imgs_hr)\n",
    "\n",
    "        # (2)Adversarial loss\n",
    "        # 本物画像と超解像画像の識別器による判定を見る\n",
    "        pred_real = self.discriminator(imgs_hr).detach()\n",
    "        pred_fake = self.discriminator(gen_hr)\n",
    "        loss_GAN = self.criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "        # (3)Perceptual loss\n",
    "        #特徴量の比較、シンプルな損失関数\n",
    "        gen_feature = self.feature_extractor(gen_hr)\n",
    "        real_feature = self.feature_extractor(imgs_hr).detach()\n",
    "        loss_content = self.criterion_content(gen_feature, real_feature)\n",
    "\n",
    "        \n",
    "        loss_G = loss_content + opt.lambda_adv * loss_GAN + opt.lambda_pixel * loss_pixel\n",
    "        loss_G.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        \n",
    "        ################### \n",
    "        #####識別機の損失#####\n",
    "        ###################\n",
    "        self.optimizer_D.zero_grad()\n",
    "        pred_real = self.discriminator(imgs_hr)\n",
    "        pred_fake = self.discriminator(gen_hr.detach())\n",
    "\n",
    "        loss_real = self.criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)            \n",
    "        loss_fake = self.criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)    \n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        loss_D.backward()\n",
    "        self.optimizer_D.step()\n",
    "\n",
    "        train_info = {'epoch': epoch, 'batch_num': batch_num,  'loss_D': loss_D.item(), 'loss_G': loss_G.item(),\n",
    "                      'loss_content': loss_content.item(), 'loss_GAN': loss_GAN.item(), 'loss_pixel': loss_pixel.item(),}\n",
    "        \"\"\"\n",
    "        if batch_num == 1:\n",
    "            sys.stdout.write('\\n{}'.format(train_info))\n",
    "        else:\n",
    "            sys.stdout.write('\\r{}'.format('\\t'*20))\n",
    "            sys.stdout.write('\\r{}'.format(train_info))\n",
    "        \"\"\"\n",
    "        sys.stdout.write('\\r{}'.format('\\t'*20))\n",
    "        sys.stdout.write('\\r{}'.format(train_info))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def save_image(self, imgs, batches_done):\n",
    "        \"\"\"\n",
    "        画像の保存\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            imgs_lr = imgs[\"lr\"].type(self.Tensor).to(self.dev)\n",
    "            gen_hr = self.generator(imgs_lr)\n",
    "            gen_hr = denormalize(gen_hr)\n",
    "\n",
    "            image_batch_save_dir = osp.join(image_test_save_dir, '{:03}'.format(i))\n",
    "            gen_hr_dir = osp.join(image_batch_save_dir, \"hr_image\")\n",
    "            os.makedirs(image_batch_save_dir, exist_ok=True)\n",
    "            save_image(gen_hr, osp.join(image_batch_save_dir, \"{:09}.png\".format(batches_done)), nrow=1, normalize=False)\n",
    "\n",
    "    def save_weight(self, batches_done):\n",
    "        \"\"\"\n",
    "        重みの保存\n",
    "        \"\"\"\n",
    "        generator_weight_path = osp.join(weight_save_dir, \"generator_{:08}.pth\".format(batches_done))\n",
    "        discriminator_weight_path = osp.join(weight_save_dir, \"discriminator_{:08}.pth\".format(batches_done))\n",
    "\n",
    "        torch.save(self.generator.state_dict(), generator_weight_path)\n",
    "        torch.save(self.discriminator.state_dict(), discriminator_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensors):\n",
    "    \"\"\"\n",
    "    平均と標準偏差を使ってデータ値の加工\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        tensors[:, c].mul_(std[c]).add_(mean[c])\n",
    "    return torch.clamp(tensors, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e2cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8daf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27087f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_save_dir = osp.join(output_dir, 'image', 'train')\n",
    "image_test_save_dir = osp.join(output_dir, 'image', 'test')\n",
    "weight_save_dir = osp.join(output_dir, 'weight')\n",
    "param_save_path = osp.join(output_dir, 'param.json')\n",
    "\n",
    "save_dirs = [image_train_save_dir, image_test_save_dir, weight_save_dir]\n",
    "for save_dir in save_dirs:\n",
    "    print(save_dir)\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d402bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(file, save_path, mode):\n",
    "    \"\"\"Jsonファイルを保存\n",
    "    \"\"\"\n",
    "    with open(save_path, mode) as outfile:\n",
    "        json.dump(file, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126179dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts():\n",
    "    def __init__(self):\n",
    "        self.n_epoch = 50\n",
    "        self.residual_blocks = 23\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.9\n",
    "        self.b2 = 0.999\n",
    "        self.batch_size = 16\n",
    "        self.n_cpu = 8\n",
    "        self.warmup_batches = 5#500\n",
    "        self.lambda_adv = 5e-3\n",
    "        self.lambda_pixel = 1e-2\n",
    "        self.pretrained = False\n",
    "        self.dataset_name = 'cat'\n",
    "        self.sample_interval = 100\n",
    "        self.checkpoint_interval = 1000\n",
    "        self.hr_height = 128\n",
    "        self.hr_width = 128\n",
    "        self.channels = 3\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        print(self.device)\n",
    "    def to_dict(self):\n",
    "        parameters = {\n",
    "            'n_epoch': self.n_epoch,\n",
    "            'hr_height': self.hr_height,\n",
    "            'residual_blocks': self.residual_blocks,\n",
    "            'lr': self.lr,\n",
    "            'b1': self.b1,\n",
    "            'b2': self.b2,\n",
    "            'batch_size': self.batch_size,\n",
    "            'n_cpu': self.n_cpu,\n",
    "            'warmup_batches': self.warmup_batches,\n",
    "            'lambda_adv': self.lambda_adv,\n",
    "            'lambda_pixel': self.lambda_pixel,\n",
    "            'pretrained': self.pretrained,\n",
    "            'dataset_name': self.dataset_name,\n",
    "            'sample_interval': self.sample_interval,\n",
    "            'checkpoint_interval': self.checkpoint_interval,\n",
    "            'hr_height': self.hr_height,\n",
    "            'hr_width': self.hr_width,\n",
    "            'channels': self.channels,\n",
    "            'device': str(self.device),\n",
    "        }\n",
    "        return parameters\n",
    "opt = Opts()\n",
    "save_json(opt.to_dict(), param_save_path, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ca563",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_shape = (opt.hr_height, opt.hr_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03615b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    ImageDataset(train_dir, hr_shape = hr_shape),\n",
    "    batch_size = opt.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = opt.n_cpu,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    TestImageDataset(test_dir),\n",
    "    batch_size = 1,\n",
    "    shuffle = False,\n",
    "    num_workers = opt.n_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "esrgan = ESRGAN(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, opt.n_epoch + 1):\n",
    "    print(\"\\ncurrent epoch : \", epoch)\n",
    "    for batch_num, imgs in enumerate(train_dataloader):\n",
    "        batches_done = (epoch - 1) * len(train_dataloader) + batch_num\n",
    "        # 事前学習\n",
    "        if batches_done <= opt.warmup_batches:#500\n",
    "            esrgan.pre_train(imgs, batch_num)\n",
    "        # メイン学習\n",
    "        else:\n",
    "            esrgan.train(imgs, batch_num)\n",
    "        # 高解像度の生成画像の保存\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            for i, imgs in enumerate(test_dataloader):\n",
    "                esrgan.save_image(imgs, batches_done)\n",
    "        # 学習した重みの保存\n",
    "        if batches_done % opt.checkpoint_interval == 0:\n",
    "            esrgan.save_weight(batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de5f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
