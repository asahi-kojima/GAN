{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e75521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22228952",
   "metadata": {},
   "source": [
    "# 教師データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
    "!mkdir -p ./datasets/facades\n",
    "!tar -zxvf facades.tar.gz -C ./datasets/\n",
    "!rm facades.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c82b0",
   "metadata": {},
   "source": [
    "## 画像の下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class generateDataset:\n",
    "    pic_extention = [\".png\", \".jpg\"]\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        dir = os.path.join(params.data_dir, params.phase)\n",
    "        self.AB_paths = sorted(self.make_dataset(dir))\n",
    "        \n",
    "    @classmethod\n",
    "    def is_image_file(self, fname):\n",
    "        return any(fname.endswith(ext) for ext in self.pic_extention)\n",
    "    \n",
    "    @classmethod\n",
    "    def make_dataset(self, dir):\n",
    "        images = []\n",
    "        assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(dir)):\n",
    "            for fname in fnames:\n",
    "                if self.is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    images.append(path)\n",
    "        return images\n",
    "    \n",
    "    def __transform(self, param):\n",
    "        list = []\n",
    "        load_num = self.params.load_num\n",
    "        list.append(transforms.Resize([load_num, load_num], Image.BICUBIC))\n",
    "\n",
    "        (x, y) = param['crop_pos']\n",
    "        crop_size = self.params.crop_size\n",
    "        list.append(transforms.Lambda(lambda img: img.crop((x, y, x + crop_size, y + crop_size))))\n",
    "\n",
    "        if param['flip']:\n",
    "            list.append(transforms.Lambda(lambda img: img.transpose(Image.FLIP_LEFT_RIGHT)))\n",
    "\n",
    "        list += [transforms.ToTensor(),\n",
    "                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "        return transforms.Compose(list)\n",
    "    \n",
    "    def __transform_param(self):\n",
    "        x_max = self.params.load_num - self.params.crop_size\n",
    "        x = random.randint(0, np.maximum(0, x_max))\n",
    "        y = random.randint(0, np.maximum(0, x_max))\n",
    "\n",
    "        flip = random.random() > 0.5\n",
    "\n",
    "        return {'crop_pos': (x, y), 'flip': flip}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        AB_path = self.AB_paths[index]\n",
    "        AB = Image.open(AB_path).convert('RGB')\n",
    "\n",
    "        param = self.__transform_param()\n",
    "        w, h = AB.size\n",
    "        w2 = int(w / 2)\n",
    "        \n",
    "        transform = self.__transform(param)\n",
    "        A = transform(AB.crop((0, 0, w2, h)))\n",
    "        B = transform(AB.crop((w2, 0, w, h)))\n",
    "\n",
    "        return {'A': B, 'B': A, 'A_paths': AB_path, 'B_paths': AB_path}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.AB_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8486e0b",
   "metadata": {},
   "source": [
    "## 識別器の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b08ef",
   "metadata": {},
   "source": [
    "### 実効的な受容野を７０とする様に設計。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98966716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 二つの画像をstackするので、チャンネルは倍\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Conv2d(6, 64, kernel_size = 4, stride = 2, padding = 1),\n",
    "        nn.LeakyReLU(0.2, True),\n",
    "        self.__nest_layer(64, 128),\n",
    "        self.__nest_layer(128, 256),\n",
    "        self.__nest_layer(256, 512, stride = 1),\n",
    "        nn.Conv2d(512, 1, kernel_size = 4, stride = 1, padding = 1)\n",
    "        )\n",
    "        \n",
    "    def __nest_layer(self, in_chn, out_chn, stride = 2):\n",
    "        #１個目の畳み込み層でサイズは半分になる。\n",
    "        layers = nn.Sequential(\n",
    "        nn.Conv2d(in_chn, out_chn, kernel_size = 4, stride = stride, padding = 1),\n",
    "        nn.BatchNorm2d(out_chn),\n",
    "        nn.LeakyReLU(0.2, True))\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbacac68",
   "metadata": {},
   "source": [
    "## 生成器の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.down0 = nn.Conv2d(3, 64, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.down1 = self.__encode_layer(64, 128)\n",
    "        self.down2 = self.__encode_layer(128, 256)\n",
    "        self.down3 = self.__encode_layer(256, 512)\n",
    "        self.down4 = self.__encode_layer(512, 512)\n",
    "        self.down5 = self.__encode_layer(512, 512)\n",
    "        self.down6 = self.__encode_layer(512, 512)\n",
    "        self.down7 = self.__encode_layer(512, 512, flag = False)\n",
    "        \n",
    "        self.up7 = self.__decode_layer(512, 512)\n",
    "        self.up6 = self.__decode_layer(1024, 512, flag_dropout = True)\n",
    "        self.up5 = self.__decode_layer(1024, 512, flag_dropout = True)\n",
    "        self.up4 = self.__decode_layer(1024, 512, flag_dropout = True)\n",
    "        self.up3 = self.__decode_layer(1024, 256)\n",
    "        self.up2 = self.__decode_layer(512, 128)\n",
    "        self.up1 = self.__decode_layer(256, 64)\n",
    "        self.up0 = nn.Sequential(\n",
    "        self.__decode_layer(128, 3, flag = False),\n",
    "        nn.Tanh())\n",
    "        \n",
    "    #エンコーダー部分\n",
    "    def __encode_layer(self, in_chn, out_chn, flag = True):\n",
    "        #畳み込みそうでサイズが半分に落ちる。\n",
    "        layers = [\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(in_chn, out_chn, kernel_size = 4, stride = 2, padding = 1)\n",
    "        ]\n",
    "        \n",
    "        if flag:\n",
    "            layers.append(nn.BatchNorm2d(out_chn))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    #デコーダー部分\n",
    "    def __decode_layer(self, in_chn, out_chn, flag = True, flag_dropout = False):\n",
    "        layers = [\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_chn, out_chn, kernel_size = 4, stride = 2, padding = 1)\n",
    "        ]\n",
    "        \n",
    "        if flag:\n",
    "            layers.append(nn.BatchNorm2d(out_chn))\n",
    "        if flag_dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.down0(x)\n",
    "        x1 = self.down1(x0)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "        x6 = self.down6(x5)\n",
    "        x7 = self.down7(x6)\n",
    "        y7 = self.up7(x7)\n",
    "        \n",
    "        y6 = self.up6(self.cat(x6, y7))\n",
    "        y5 = self.up5(self.cat(x5, y6))\n",
    "        y4 = self.up4(self.cat(x4, y5))\n",
    "        y3 = self.up3(self.cat(x3, y4))\n",
    "        y2 = self.up2(self.cat(x2, y3))\n",
    "        y1 = self.up1(self.cat(x1, y2))\n",
    "        y0 = self.up0(self.cat(x0, y1))\n",
    "        \n",
    "        return y0\n",
    "    \n",
    "    \n",
    "    def cat(self, x,y):\n",
    "        return torch.cat([x,y], dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58589c95",
   "metadata": {},
   "source": [
    "## 損失関数の導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ac76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pix2pix_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.register_buffer(\"real\", torch.tensor(1.0))\n",
    "        self.register_buffer(\"fake\", torch.tensor(0.0))\n",
    "        \n",
    "    def __call__(self,prediction, is_real):\n",
    "        if is_real :\n",
    "            target_tensor = self.real\n",
    "        else:\n",
    "            target_tensor = self.fake\n",
    "            \n",
    "        return self.loss(prediction, target_tensor.expand_as(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a2880",
   "metadata": {},
   "source": [
    "# モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b62034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2pix:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        \n",
    "        #生成器\n",
    "        self.Gnet = Generator().to(self.params.device)\n",
    "        self.Gnet.apply(self.weights_init)\n",
    "        if self.params.path2generator != None:\n",
    "            self.Gnet.load_state_dict(torch.load(self.params.path2generator, \n",
    "                                                 map_location=self.params.device_name), strict=False)\n",
    "        #識別器\n",
    "        self.Dnet = Discriminator().to(self.params.device)\n",
    "        self.Dnet.apply(self.weights_init)\n",
    "        if self.params.path2discriminator != None:\n",
    "            self.Dnet.load_state_dict(torch.load(self.params.path2discriminator, \n",
    "                                                 map_location=self.params.device_name), strict=False)\n",
    "        #オプティマイザー\n",
    "        self.optimizerG = optim.Adam(self.Gnet.parameters(), \n",
    "                                                         lr = 0.00002, betas = (0.5, 0.999))\n",
    "        self.optimizerD = optim.Adam(self.Dnet.parameters(), \n",
    "                                                         lr = 0.00002, betas = (0.5, 0.999))\n",
    "        \n",
    "        #損失関数\n",
    "        self.loss_pixel = pix2pix_Loss().to(self.params.device)\n",
    "        self.loss_L1 = nn.L1Loss()\n",
    "        \n",
    "        #スケジューラー\n",
    "        self.schedulerG = optim.lr_scheduler.LambdaLR(self.optimizerG, \n",
    "                                                                                  self.__change_lr)\n",
    "        self.schedulerD = optim.lr_scheduler.LambdaLR(self.optimizerD, \n",
    "                                                                                  self.__change_lr)\n",
    "        \n",
    "    def update_lr(self):\n",
    "        self.schedulerG.step()\n",
    "        self.schedulerD.step()\n",
    "        \n",
    "    def __change_lr(self, epoch):\n",
    "        if self.params.epochs_lr_decay_start < 0:\n",
    "            return 1.\n",
    "        delta = max(0, epoch - self.params.epochs_lr_decay_start) / float(self.params.epochs_lr_decay)\n",
    "        return max(0.0, 1.0 - delta)\n",
    "    \n",
    "    \n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            m.weight.data.normal_(0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)\n",
    "            \n",
    "    def train(self, data, batches_done):\n",
    "        self.realA = data[\"A\"].to(self.params.device)\n",
    "        self.realB = data[\"B\"].to(self.params.device)\n",
    "        \n",
    "        #realA --> fakeB\n",
    "        fakeB = self.Gnet(self.realA)\n",
    "        self.fakeB = fakeB\n",
    "        \n",
    "        #本物と偽物のペアの損失\n",
    "        real_fake = torch.cat( (self.realA, fakeB) , dim = 1)\n",
    "        prediction_fake = self.Dnet(real_fake.detach())\n",
    "        loss_fake = self.loss_pixel(prediction_fake, False)\n",
    "        \n",
    "        #本物画像のペアの損失\n",
    "        real_real = torch.cat( (self.realA, self.realB) , dim = 1)\n",
    "        prediction_real = self.Dnet(real_real)\n",
    "        loss_real = self.loss_pixel(prediction_real, True)\n",
    "        \n",
    "        #合計損失\n",
    "        lossD = 0.5 * (loss_real + loss_fake)\n",
    "        \n",
    "        \n",
    "        #識別器の更新\n",
    "        self.optimizerD.zero_grad()\n",
    "        lossD.backward()\n",
    "        self.optimizerD.step()\n",
    "        \n",
    "        #生成器の損失計算\n",
    "        with torch.no_grad():\n",
    "            prediction_fake = self.Dnet(real_fake)\n",
    "        lossG_pix = self.loss_pixel(prediction_fake, True)\n",
    "        lossG_L1 = self.loss_L1(fakeB, self.realB)\n",
    "        lossG = lossG_pix + lossG_L1 * self.params.lambda_L1\n",
    "        \n",
    "        #生成器の更新\n",
    "        self.optimizerG.zero_grad()\n",
    "        lossG.backward()\n",
    "        self.optimizerG.step()\n",
    "        \n",
    "        return lossD, lossG\n",
    "        \n",
    "    def save_model(self, epoch):\n",
    "        # モデルの保存\n",
    "        output_dir = self.params.output_dir\n",
    "        torch.save(self.Gnet.state_dict(), '{}/pix2pix_G_epoch_{}'.format(output_dir, epoch))\n",
    "        torch.save(self.Dnet.state_dict(), '{}/pix2pix_D_epoch_{}'.format(output_dir, epoch))\n",
    "\n",
    "    def save_image(self, epoch):\n",
    "        output_image = torch.cat([self.realA, self.fakeB, self.realB], dim=3)\n",
    "        vutils.save_image(output_image,\n",
    "                '{}/pix2pix_epoch_{}.png'.format(self.params.output_dir, epoch),\n",
    "                normalize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82b025",
   "metadata": {},
   "source": [
    "## 細かい準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_json(file, save_path, mode):\n",
    "    with open(param_save_path, mode) as outfile:\n",
    "        json.dump(file, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec53b8",
   "metadata": {},
   "source": [
    "## パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c878c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter_set:\n",
    "    def __init__(self):\n",
    "        self.epochs = 100\n",
    "        \n",
    "        self.save_data_interval = 10\n",
    "        self.save_image_interval = 1\n",
    "        self.sample_interval = 10\n",
    "        \n",
    "        self.batch_num = 64\n",
    "        self.load_num = 286\n",
    "        self.crop_size = 256\n",
    "        \n",
    "        self.cpu = True \n",
    "        self.data_dir = 'datasets/facades'\n",
    "        self.output_dir = 'output'\n",
    "        self.phase = 'train'\n",
    "        \n",
    "        self.lambda_L1 = 100.\n",
    "        self.epochs_lr_decay = 0 \n",
    "        self.epochs_lr_decay_start = -1 \n",
    "        self.path2generator = None\n",
    "        self.path2discriminator = None\n",
    "        self.device_name = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = torch.device(self.device_name)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def dict(self):\n",
    "        params = {\n",
    "            'epochs': self.epochs,\n",
    "            'save_data_interval': self.save_data_interval,\n",
    "            'save_image_interval': self.save_image_interval,\n",
    "            'sample_interval': self.sample_interval,\n",
    "            'batch_num': self.batch_num,\n",
    "            'load_num': self.load_num,\n",
    "            'crop_size': self.crop_size,\n",
    "            'cpu': self.cpu,\n",
    "            'data_dir': self.data_dir,\n",
    "            'output_dir': self.output_dir,\n",
    "            'phase': self.phase,\n",
    "            'lambda_L1': self.lambda_L1,\n",
    "            'epochs_lr_decay': self.epochs_lr_decay,\n",
    "            'epochs_lr_decay_start': self.epochs_lr_decay_start,\n",
    "            'path2generator': self.path2generator,\n",
    "            'path2discriminator': self.path2discriminator,\n",
    "            'device_name': self.device_name\n",
    "        }\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39fccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_set = Parameter_set()\n",
    "param_save_path = os.path.join('output', 'param.json')\n",
    "save_json(params_set.dict(), param_save_path, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e8a49",
   "metadata": {},
   "source": [
    "## モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pix2pix(params_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38453e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generateDataset(params_set)\n",
    "dataloader = DataLoader(dataset, batch_size = params_set.batch_num, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04625ca3",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, params_set.epochs + 1):\n",
    "    for batch_num, data in enumerate(dataloader):\n",
    "        batches_done = (epoch - 1) * len(dataloader) + batch_num\n",
    "        loss1, loss2 = model.train(data, batches_done)\n",
    "        \n",
    "        if batch_num % 20 == 0:\n",
    "            print(\"Current Epoch {} :  Loss_D: {:.4f} Loss_G: {:.4f}\".format(epoch, loss1, loss2))\n",
    "\n",
    "    if epoch % params_set.save_data_interval == 0:\n",
    "        model.save_model(epoch)\n",
    "\n",
    "    if epoch % params_set.save_image_interval == 0:\n",
    "        model.save_image(epoch)\n",
    "\n",
    "    model.update_lr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
